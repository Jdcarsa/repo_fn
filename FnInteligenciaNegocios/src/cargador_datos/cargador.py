"""
M√≥dulo para la carga de datos desde los archivos originales.
Incluye correcciones y limpiezas necesarias durante la carga.
"""

import pandas as pd
from pathlib import Path
from config.config_loader import ConfigLoader
from src.utilidades.logger import configurar_logger

logger = configurar_logger("finnovarisk.carga_datos")
CONFIG_LOADER = ConfigLoader()
CONFIG_CARGA = CONFIG_LOADER.get_config()


def limpiar_identificadores_al_cargar(df: pd.DataFrame, nombre_dataset: str) -> pd.DataFrame:
    """
    Limpia columnas de identificadores al momento de cargar.
    Convierte floats como 1007518.0 a strings "1007518"
    """
    # Columnas que deben ser limpiadas en TODOS los datasets
    columnas_identificadores = [
        'cedula', 'cc_nit', 'nit', 'vinculado', 'identificacion',
        'numero', 'dsm_num', 'ds_numero', 'mcnnumcru2', 'obligacion'
    ]
    
    columnas_limpiadas = []
    
    for col in df.columns:
        # Buscar columnas que coincidan (case-insensitive)
        col_lower = col.lower()
        
        if any(identificador in col_lower for identificador in columnas_identificadores):
            # Esta columna necesita limpieza
            if df[col].dtype in ['float64', 'float32', 'int64', 'int32']:
                # Convertir a string limpio
                df[col] = df[col].apply(lambda x: str(int(x)) if pd.notna(x) and x == int(x) else str(x) if pd.notna(x) else "")
                columnas_limpiadas.append(col)
    
    if columnas_limpiadas:
        logger.info(f"  üßπ Limpiadas {len(columnas_limpiadas)} columnas de identificadores")
        logger.debug(f"     Columnas: {columnas_limpiadas}")
    
    return df

def cargar_excel_con_config(ruta: Path, hojas_config: dict, 
                            nombre_dataset: str = "") -> list[pd.DataFrame]:
    """
    Carga las hojas especificadas de un archivo Excel.
    Aplica correcciones espec√≠ficas seg√∫n el dataset y la hoja.
    
    Args:
        ruta: Ruta al archivo Excel
        hojas_config: Dict con {nombre_hoja: fecha_corte}
        nombre_dataset: Nombre del dataset para aplicar correcciones espec√≠ficas
        
    Returns:
        Lista de DataFrames cargados
    """
    dataframes = []
    if not ruta.exists():
        logger.error(f"‚ùå Archivo no encontrado: {ruta}")
        return dataframes

    try:
        xls = pd.ExcelFile(ruta)
        
        for hoja, fecha_str in hojas_config.items():
            if hoja not in xls.sheet_names:
                logger.warning(f"‚ö†Ô∏è  Hoja '{hoja}' no encontrada en {ruta.name}")
                continue
            
            # Cargar la hoja
            df = pd.read_excel(xls, sheet_name=hoja)
            logger.info(f"üìÑ Cargando hoja: {hoja} - {len(df):,} registros")
            # LIMPIAR COLUMNAS QUE DEBER√çAN SER STRING (cedula, numero, nit, etc)
            df = limpiar_identificadores_al_cargar(df, nombre_dataset)
            # === CORRECCIONES ESPEC√çFICAS DURANTE LA CARGA ===
            
            # CORRECCI√ìN 1: FNZ007 - Eliminar columnas 43-88 en ciertos cortes
            if nombre_dataset == "FNZ007":
                df = corregir_columnas_fnz007(df, hoja, ruta.name)
            
            # CORRECCI√ìN 2: FNZ007 - Corregir formato de fecha de nacimiento
            if nombre_dataset == "FNZ007":
                df = corregir_fecha_nacimiento_fnz007(df, hoja)
            
            # CORRECCI√ìN 3: ANALISIS_CARTERA - Eliminar 'fechadoc' en ciertos cortes
            if nombre_dataset == "ANALISIS_CARTERA":
                df = corregir_fechadoc_ac(df, hoja)
            
            # Agregar fecha de corte si existe
            if fecha_str is not None:
                df["corte"] = pd.to_datetime(fecha_str)
            
            dataframes.append(df)
            logger.info(f"  ‚úÖ Hoja procesada: {len(df):,} registros, {len(df.columns)} columnas")
    
    except Exception as e:
        logger.exception(f"‚ùå Error cargando {ruta}: {e}")
    
    return dataframes


def corregir_columnas_fnz007(df: pd.DataFrame, hoja: str, archivo: str) -> pd.DataFrame:
    """
    Elimina columnas extras (43-88) en ciertos cortes de FNZ007.
    
    Seg√∫n R:
    abr24, jul24, jun24, may24 tienen columnas extras que deben eliminarse.
    """
    # Identificar cortes problem√°ticos
    cortes_con_columnas_extras = [
        "ABRIL 24", "MAYO 24", "JUNIO 24", "JULIO 24"
    ]
    
    if hoja in cortes_con_columnas_extras:
        num_columnas_antes = len(df.columns)
        
        # Verificar si hay m√°s de 88 columnas
        if num_columnas_antes > 88:
            # Eliminar columnas 43-88 (√≠ndices 42-87 en Python)
            columnas_a_mantener = list(range(0, 42)) + list(range(88, num_columnas_antes))
            df = df.iloc[:, columnas_a_mantener]
            
            num_columnas_despues = len(df.columns)
            columnas_eliminadas = num_columnas_antes - num_columnas_despues
            
            logger.info(f"  üîß {hoja}: Eliminadas {columnas_eliminadas} columnas extras (43-88)")
            logger.info(f"     Columnas: {num_columnas_antes} ‚Üí {num_columnas_despues}")
        else:
            logger.info(f"  ‚ÑπÔ∏è  {hoja}: No tiene columnas extras para eliminar")
    
    return df


def corregir_fecha_nacimiento_fnz007(df: pd.DataFrame, hoja: str) -> pd.DataFrame:
    """
    Corrige el formato de la columna FS1NACFEC en ciertos cortes.
    
    Seg√∫n R:
    dic23$FS1NACFEC <- as.POSIXct(dic23$FS1NACFEC, format = "%Y-%m-%d %H:%M:%S")
    ago24$FS1NACFEC <- as.POSIXct(ago24$FS1NACFEC, format = "%Y-%m-%d %H:%M:%S")
    """
    cortes_con_formato_diferente = ["DICIEMBRE 23", "AGOSTO 24"]
    
    if hoja in cortes_con_formato_diferente:
        if 'FS1NACFEC' in df.columns:
            try:
                # Convertir a datetime con formato espec√≠fico
                df['FS1NACFEC'] = pd.to_datetime(
                    df['FS1NACFEC'], 
                    format='%Y-%m-%d %H:%M:%S',
                    errors='coerce'
                )
                
                nulos = df['FS1NACFEC'].isnull().sum()
                logger.info(f"  üîß {hoja}: Fecha nacimiento corregida ({nulos} nulos)")
            except Exception as e:
                logger.warning(f"  ‚ö†Ô∏è  {hoja}: No se pudo corregir fecha nacimiento: {e}")
    
    return df


def corregir_fechadoc_ac(df: pd.DataFrame, hoja: str) -> pd.DataFrame:
    """
    Corrige columnas seg√∫n el corte temporal en An√°lisis de Cartera.
    
    Correcciones:
    1. Elimina 'fechadoc' en ciertos cortes espec√≠ficos
    2. Normaliza nombres de columna de d√≠as de atraso (busca todas las variaciones)
    
    Maneja variaciones:
    - 'DIAS ATRASO', 'DIAS ATRASOS', 'diasatras', 'Dias Atraso', etc.
    - B√∫squeda case-insensitive
    - Con/sin tildes, con/sin espacios
    """
    # Cortes que requieren eliminaci√≥n de fechadoc
    cortes_con_fechadoc = ["AGOSTO 24", "FEBRERO 25", "MARZO 25", "SEPTIEMBRE 25"]
    cortes_con_FACTURA = ['OCTUBRE 25']
    # === CORRECCI√ìN 1: ELIMINAR FECHADOC ===
    hoja_upper = hoja.upper().strip()
    
    if hoja_upper in [corte.upper() for corte in cortes_con_fechadoc]:
        # Buscar columna fechadoc (case-insensitive)
        cols_fechadoc = [col for col in df.columns if col.lower() == 'fechadoc']
        
        if cols_fechadoc:
            df = df.drop(columns=cols_fechadoc)
            logger.info(f"  üîß {hoja}: Eliminada columna '{cols_fechadoc[0]}'")
        else:
            logger.info(f"  ‚ÑπÔ∏è  {hoja}: No tiene columna 'fechadoc'")
    
    if hoja_upper in [corte.upper() for corte in cortes_con_FACTURA]:
        
        # Buscar columna FACTURA (case-insensitive)
        cols_factura = [col for col in df.columns if col.lower() == 'factura']
        
        if cols_factura:
            df = df.drop(columns=cols_factura)
            logger.info(f"  üîß {hoja}: Eliminada columna '{cols_factura[0]}'")
        else:
            logger.info(f"  ‚ÑπÔ∏è  {hoja}: No tiene columna 'FACTURA'")
    # === CORRECCI√ìN 2: NORMALIZAR DIAS ATRASO ===
    # Buscar TODAS las variaciones posibles (case-insensitive, con/sin espacio)
    col_dias_atraso = None
    
    # Patrones a buscar (de m√°s espec√≠fico a m√°s general)
    patrones_busqueda = [
        'DIAS ATRASO',      # Con espacio (m√°s com√∫n en cortes antiguos)
        'DIAS ATRASOS',     # Plural
        'DIASATRASO',       # Sin espacio
        'DIASATRAS',        # Abreviado
        'diasatras',        # Ya normalizado
        'DIAS_ATRASO',      # Con guion bajo
        'dias atraso',      # Min√∫sculas con espacio
        'Dias Atraso',      # Title case
    ]
    
    # Buscar la columna (case-insensitive, ignorando espacios extra)
    for col in df.columns:
        col_normalizado = col.upper().strip().replace('_', ' ')
        
        for patron in patrones_busqueda:
            patron_normalizado = patron.upper().strip().replace('_', ' ')
            
            if col_normalizado == patron_normalizado:
                col_dias_atraso = col
                break
        
        if col_dias_atraso:
            break
    
    # Renombrar si se encontr√≥ y no es el nombre correcto
    if col_dias_atraso:
        if col_dias_atraso != 'diasatras':
            df = df.rename(columns={col_dias_atraso: 'diasatras'})
            logger.info(f"  üîß {hoja}: Renombrada '{col_dias_atraso}' ‚Üí 'diasatras'")
        else:
            logger.info(f"  ‚úÖ {hoja}: Ya tiene 'diasatras'")
    else:
        # No se encontr√≥ - ADVERTENCIA
        logger.warning(f"  ‚ö†Ô∏è  {hoja}: No se encontr√≥ columna de d√≠as de atraso")
        logger.warning(f"      Columnas disponibles: {list(df.columns)[:15]}")
    
    return df


def concatenar_dataframes(dataframes: list[pd.DataFrame], nombre: str) -> pd.DataFrame:
    """
    Concatena un conjunto de DataFrames y lo loguea.
    """
    if not dataframes:
        logger.warning(f"‚ö†Ô∏è  Sin datos cargados para {nombre}")
        return pd.DataFrame()
    
    # Verificar que todas las hojas tengan las mismas columnas
    if len(dataframes) > 1:
        cols_primera = set(dataframes[0].columns)
        
        for i, df in enumerate(dataframes[1:], 1):
            cols_actual = set(df.columns)
            
            # Columnas faltantes y extras
            faltantes = cols_primera - cols_actual
            extras = cols_actual - cols_primera
            
            if faltantes or extras:
                logger.warning(f"  ‚ö†Ô∏è  Hoja {i+1} tiene diferencias en columnas:")
                if faltantes:
                    logger.warning(f"     Faltantes: {list(faltantes)[:5]}")
                if extras:
                    logger.warning(f"     Extras: {list(extras)[:5]}")
    
    # Concatenar
    df_total = pd.concat(dataframes, ignore_index=True)
    
    logger.info("")
    logger.info("="*70)
    logger.info(f"üìä CONCATENACI√ìN COMPLETA: {nombre}")
    logger.info("="*70)
    logger.info(f"Total hojas concatenadas: {len(dataframes)}")
    logger.info(f"Total registros: {len(df_total):,}")
    logger.info(f"Total columnas: {len(df_total.columns)}")
    logger.info(f"Memoria: {df_total.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
    
    # Rango de fechas si existe columna 'corte'
    if 'corte' in df_total.columns:
        fecha_min = df_total['corte'].min()
        fecha_max = df_total['corte'].max()
        logger.info(f"Rango de cortes: {fecha_min} ‚Üí {fecha_max}")
    
    logger.info("="*70)
    logger.info("")
    
    return df_total


def cargar_dataset(nombre: str) -> pd.DataFrame:
    """
    Carga un dataset completo seg√∫n la configuraci√≥n global.
    Aplica correcciones espec√≠ficas durante la carga.

    Args:
        nombre: nombre del dataset (ej. 'FNZ007')

    Returns:
        DataFrame concatenado y corregido
    """
    logger.info("")
    logger.info("="*70)
    logger.info(f"üöÄ INICIANDO CARGA: {nombre}")
    logger.info("="*70)
    
    if nombre not in CONFIG_CARGA:
        logger.error(f"‚ùå No existe configuraci√≥n para '{nombre}'")
        return pd.DataFrame()

    cfg = CONFIG_CARGA[nombre]
    dataframes = []

    for clave_archivo, ruta in cfg["archivos"].items():
        logger.info(f"\nüìÅ Archivo: {ruta.name}")
        
        hojas = cfg["hojas"].get(clave_archivo, {})
        if not hojas:
            logger.warning(f"‚ö†Ô∏è  No hay hojas definidas para {clave_archivo}")
            continue
        
        # Pasar el nombre del dataset para aplicar correcciones espec√≠ficas
        dataframes.extend(
            cargar_excel_con_config(ruta, hojas, nombre_dataset=nombre)
        )

    return concatenar_dataframes(dataframes, nombre)


def cargar_auxiliares() -> dict[str, pd.DataFrame]:
    """
    Carga archivos auxiliares (sin fecha de corte).
    
    Returns:
        dict[str, pd.DataFrame]: Diccionario con DataFrames auxiliares
    """
    logger.info("")
    logger.info("="*70)
    logger.info("üöÄ CARGANDO ARCHIVOS AUXILIARES")
    logger.info("="*70)
    
    auxiliares = {}
    
    if "AUXILIARES" not in CONFIG_CARGA:
        logger.error("‚ùå No hay configuraci√≥n para AUXILIARES")
        return auxiliares
        
    cfg = CONFIG_CARGA["AUXILIARES"]
    
    for clave_archivo, ruta in cfg["archivos"].items():
        logger.info(f"\nüìÑ Cargando: {clave_archivo}")
        logger.info(f"   Archivo: {ruta.name}")
        
        if not ruta.exists():
            logger.error(f"   ‚ùå No encontrado: {ruta}")
            continue
        
        try:
            df = pd.read_excel(ruta)
            
            # Normalizar nombres de columnas
            df.columns = [col.lower().strip() for col in df.columns]
            
            auxiliares[clave_archivo] = df
            
            logger.info(f"   ‚úÖ Cargado: {len(df):,} registros, {len(df.columns)} columnas")
            logger.info(f"   Columnas: {list(df.columns)[:5]}...")
            
        except Exception as e:
            logger.exception(f"   ‚ùå Error: {e}")
    
    logger.info("")
    logger.info("="*70)
    logger.info(f"‚úÖ AUXILIARES CARGADOS: {len(auxiliares)} archivos")
    logger.info("="*70)
    logger.info("")
    
    return auxiliares


# === Funciones de compatibilidad ===

def cargar_fnz007() -> pd.DataFrame:
    """Carga FNZ007 con todas las correcciones."""
    return cargar_dataset("FNZ007")


def cargar_analisis_cartera() -> pd.DataFrame:
    """Carga An√°lisis de Cartera."""
    return cargar_dataset("ANALISIS_CARTERA")


def cargar_fnz001() -> pd.DataFrame:
    """Carga FNZ001."""
    return cargar_dataset("FNZ001")


def cargar_edades() -> pd.DataFrame:
    """Carga Edades."""
    return cargar_dataset("EDADES")


def cargar_r05() -> pd.DataFrame:
    """Carga R05."""
    return cargar_dataset("R05")


def cargar_recaudos() -> pd.DataFrame:
    """Carga Recaudos."""
    return cargar_dataset("RECAUDOS")


# === FUNCI√ìN DE VALIDACI√ìN POST-CARGA ===

def validar_carga_completa() -> dict:
    """
    Valida que todos los datasets cr√≠ticos se hayan cargado correctamente.
    
    Returns:
        dict con resultados de validaci√≥n
    """
    logger.info("")
    logger.info("="*70)
    logger.info("üîç VALIDACI√ìN DE CARGA COMPLETA")
    logger.info("="*70)
    
    datasets_criticos = ["FNZ007", "ANALISIS_CARTERA", "FNZ001"]
    datasets_opcionales = ["EDADES", "R05", "RECAUDOS"]
    
    resultados = {
        "criticos_ok": True,
        "problemas": [],
        "advertencias": []
    }
    
    # Validar cr√≠ticos
    for dataset in datasets_criticos:
        try:
            df = cargar_dataset(dataset)
            if df is None or len(df) == 0:
                resultados["criticos_ok"] = False
                resultados["problemas"].append(f"{dataset}: Vac√≠o o None")
                logger.error(f"   ‚ùå {dataset}: FALLO")
            else:
                logger.info(f"   ‚úÖ {dataset}: {len(df):,} registros")
        except Exception as e:
            resultados["criticos_ok"] = False
            resultados["problemas"].append(f"{dataset}: {str(e)}")
            logger.error(f"   ‚ùå {dataset}: ERROR - {e}")
    
    # Validar opcionales
    for dataset in datasets_opcionales:
        try:
            df = cargar_dataset(dataset)
            if df is None or len(df) == 0:
                resultados["advertencias"].append(f"{dataset}: Vac√≠o o None")
                logger.warning(f"   ‚ö†Ô∏è  {dataset}: Sin datos")
            else:
                logger.info(f"   ‚úÖ {dataset}: {len(df):,} registros")
        except Exception as e:
            resultados["advertencias"].append(f"{dataset}: {str(e)}")
            logger.warning(f"   ‚ö†Ô∏è  {dataset}: ERROR - {e}")
    
    # Validar auxiliares
    try:
        aux = cargar_auxiliares()
        if len(aux) == 0:
            resultados["advertencias"].append("No se cargaron auxiliares")
            logger.warning(f"   ‚ö†Ô∏è  AUXILIARES: Sin archivos")
        else:
            logger.info(f"   ‚úÖ AUXILIARES: {len(aux)} archivos")
    except Exception as e:
        resultados["advertencias"].append(f"Auxiliares: {str(e)}")
        logger.warning(f"   ‚ö†Ô∏è  AUXILIARES: ERROR - {e}")
    
    logger.info("="*70)
    
    if resultados["criticos_ok"]:
        logger.info("‚úÖ VALIDACI√ìN EXITOSA - Todos los datasets cr√≠ticos cargados")
    else:
        logger.error("‚ùå VALIDACI√ìN FALLIDA - Hay problemas en datasets cr√≠ticos")
    
    if resultados["advertencias"]:
        logger.warning(f"‚ö†Ô∏è  {len(resultados['advertencias'])} advertencias")
    
    logger.info("")
    
    return resultados